# ğŸ¤Ÿ AI-Powered Word-Level Sign Language Interpreter

A real-time sign language gesture recognition system that interprets **word-level gestures** like *Hi*, *Thank You*, *I Love You*, etc., using a webcam, MediaPipe, and machine learning â€” all wrapped in a simple and aesthetic GUI.

## ğŸ“Œ Project Highlights
- ğŸ” Real-time gesture recognition using webcam
- âœ‹ Tracks 21 hand landmarks with MediaPipe
- ğŸ¤– Machine learning classification (Random Forest)
- ğŸ¨ Python Tkinter GUI with emoji feedback
- ğŸ› ï¸ Custom training data with live capture
- ğŸ§© Easily extendable to more gestures

## ğŸ¯ Recognized Gestures
- Hi ğŸ‘‹
- Thank You ğŸ™
- Bye ğŸ‘‹
- Yes ğŸ‘
- No ğŸ‘
- I Love You ğŸ¤Ÿ

## ğŸ§ª How It Works
1. **Data Collection**  
   Collect hand gesture samples using webcam & save them as CSV files.

2. **Model Training**  
   Train a classifier on the hand landmark data (63 features per sample).

3. **Real-Time Prediction**  
   Run the interpreter to recognize gestures live from webcam feed.

4. **GUI Display**  
   Recognized gestures are shown with labels & emojis via a Python Tkinter interface.

## âš™ï¸ Tech Stack
- Python 3.10+
- MediaPipe
- OpenCV
- scikit-learn
- Tkinter
- joblib

## ğŸš€ Setup Instructions
```bash
# 1. Clone the repo
git clone https://github.com/yourusername/ai-sign-language-interpreter

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the GUI interpreter
python interpreter_gui.py
```

> âœ¨ Make sure your webcam is connected and working!

## ğŸ“‚ File Structure
```
â”œâ”€â”€ collect_data.py
â”œâ”€â”€ merge_data.py
â”œâ”€â”€ train_model.py
â”œâ”€â”€ interpreter_gui.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gesture_model.pkl
â”‚   â””â”€â”€ label_encoder.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ gesture_data_hi.csv
â”‚   â”œâ”€â”€ gesture_data_no.csv
â”‚   â””â”€â”€ gesture_data_all.csv
â””â”€â”€ README.md
```

## ğŸ”’ License
This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.

## ğŸ™ Credits
- Google MediaPipe
- scikit-learn documentation
- OpenCV
- Community tutorials on YouTube & GitHub

## ğŸ’¡ Future Scope
- Add text-to-speech output
- Enable multi-hand gesture support
- Let users train new gestures via GUI
- Gesture chaining to form full sentences
- Mobile or web app deployment

## ğŸ“ Contact
@tpriyal2016@gmail.com 

Drop a â­ if you find this helpful!
